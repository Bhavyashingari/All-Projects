{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[**Biomedical image segmentation with PyTorch and U-Net**](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Introduction](#toc0_)\n",
    "\n",
    "Imagine you work for a hospital's radiology department. The hospital recently acquired a large dataset of MRI scans to detect brain tumors, and the current manual segmentation process is both time-consuming and prone to variability between analysts. This inconsistency can lead to delays and errors in diagnosis, affecting patient care.\n",
    "\n",
    "To address this issue, you decide to implement a deep learning-based solution for automated image segmentation. By leveraging the power of PyTorch and the U-Net architecture, you aim to develop a model that can accurately and efficiently segment medical images, providing consistent and reliable results. This not only speeds up the diagnostic process but also reduces the workload on medical professionals, allowing them to focus on critical aspects of patient care.\n",
    "\n",
    "In this project, you'll launch on a journey to build and train a U-Net model for biomedical image segmentation. By the end of this tutorial, you'll have a robust understanding of how to preprocess biomedical images, construct a U-Net model, and train it to achieve high accuracy in segmenting medical images, thus showcasing the transformative potential of deep learning in medical diagnostics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[What does this guided project do?](#toc0_)\n",
    "\n",
    "This project is a comprehensive, step-by-step guide to building a biomedical image segmentation model using PyTorch and the U-Net architecture. It begins with data preprocessing, where you'll learn essential techniques for handling biomedical images, including normalization, resizing, and ensuring your data is well-prepared for model training.\n",
    "\n",
    "You will then construct the U-Net model, understanding and implementing its key components: convolutional layers for feature extraction, max-pooling layers for downsampling, and upsampling layers for precise segmentation. You'll explore how each part contributes to the model's ability to capture fine details and segment complex structures in medical images.\n",
    "\n",
    "Training the U-Net model is the next focus. This hands-on experience will solidify your understanding of deep learning practices in biomedical image segmentation.\n",
    "\n",
    "By the end of this tutorial, you'll be equipped with the skills and knowledge to develop and deploy a U-Net model for biomedical image segmentation, enhancing diagnostic accuracy and contributing to better healthcare outcomes.\n",
    "\n",
    "This tutorial is perfect for biomedical engineering or data science professionals looking to apply deep learning techniques in the medical imaging domain, leveraging PyTorch and U-Net to tackle real-world challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "  \n",
    "  - [Introduction](#toc1_1_)    \n",
    "  - [What does this guided project do?](#toc1_2_)    \n",
    "  - [Background](#toc1_3_)    \n",
    "    - [What is image segmentation](#toc1_3_1_)    \n",
    "    - [What is PyTorch](#toc1_3_2_)    \n",
    "    - [What is U-Net](#toc1_3_3_)    \n",
    "  - [Objectives](#toc1_4_)    \n",
    "  - [Setup](#toc1_5_)    \n",
    "    - [Installing Required Libraries](#toc1_5_1_)    \n",
    "    - [Importing Required Libraries](#toc1_5_2_)    \n",
    "    - [Import data](#toc1_5_3_)    \n",
    "    - [U-Net Architecture](#toc1_5_4_)    \n",
    "      - [Encoder (Contracting Path)](#toc1_5_4_1_)    \n",
    "      - [Decoder (Expanding Path)](#toc1_5_4_2_)    \n",
    "      - [Output Layer](#toc1_5_4_3_)    \n",
    "      - [Advantages of U-Net Architecture](#toc1_5_4_4_)    \n",
    "    - [Display the U-Net Model Structure](#toc1_5_5_)    \n",
    "    - [Training the U-Net Model](#toc1_5_6_)    \n",
    "    - [Loading and Testing a Pre-trained U-Net Model](#toc1_5_7_)    \n",
    "  - [Exercises](#toc1_6_)    \n",
    "    - [Exercise - Try different test images](#toc1_6_1_)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Background](#toc0_)\n",
    "\n",
    "### <a id='toc1_3_1_'></a>[What is image segmentation](#toc0_)\n",
    "\n",
    "Image segmentation is a fundamental technique in computer vision that involves dividing a digital image into multiple segments or sets of pixels, often referred to as superpixels. The objective is to simplify the image to make it easier to analyze by transforming it into something that is more meaningful and easier to interpret.\n",
    "\n",
    "**Types of Image Segmentation**\n",
    "\n",
    "- **Semantic Segmentation**: Assigns a label to every pixel in an image such that pixels with the same label share certain characteristics. All pixels that belong to a specific object (like a road, car, or tree) are assigned the same label.\n",
    "\n",
    "- **Instance Segmentation**: Goes a step beyond semantic segmentation by not only assigning each pixel a label but also distinguishing between different instances of the same type of object. For instance, it can differentiate between two cars of the same model in an image.\n",
    "\n",
    "- **Panoptic Segmentation**: Merges the concepts of semantic and instance segmentation, classifying all pixels into a known category and differentiating between individual instances of the same category.\n",
    "\n",
    "### <a id='toc1_3_2_'></a>[What is PyTorch](#toc0_)\n",
    "\n",
    "[PyTorch](https://pytorch.org/) is an open-source machine learning library, developed by Facebook's AI Research lab (FAIR). It is primarily used for applications in areas such as computer vision and natural language processing.\n",
    "\n",
    "**Common uses of PyTorch**\n",
    "\n",
    "- **Developing deep learning models**: From standard feed-forward networks to complex neural networks like CNNs and RNNs.\n",
    "- **Research and experimentation**: Facilitates rapid prototyping, which is highly valued in academic and research settings.\n",
    "- **Production deployment**: With the support of TorchServe, PyTorch models can be easily transitioned from research to production environments.\n",
    "\n",
    "\n",
    "### <a id='toc1_3_3_'></a>[What is U-Net](#toc0_)\n",
    "\n",
    "**U-Net** is a fully convolutional neural network originally designed for semantic image segmentation of biomedical images. This network features a distinctive encoder-decoder structure, making it exceptionally effective for detailed and precise image analysis tasks in the medical field. Since its inception, U-Net has been widely recognized for its robust performance in segmenting complex anatomical structures in medical scans, such as MRIs and CT images.\n",
    "\n",
    "The architecture of U-Net is particularly notable for its ability to capture contextual information at various scales, which is crucial for accurate segmentation. The encoder part of the network progressively reduces the spatial dimension of the image, capturing high-level semantic information. Conversely, the decoder part gradually recovers object details and spatial dimensions, ensuring precise localization.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Kk-yKGkvRtkNzLkZdP3Pag/unet.png\" width=\"70%\" alt=\"indexing\"/>\n",
    "\n",
    "**Reference:**\n",
    "1. Original Paper: [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Objectives](#toc0_)\n",
    "\n",
    "After you complete the project, you will:\n",
    "\n",
    "- **Understand the basics of biomedical image segmentation**: Learn the importance and applications of image segmentation in the medical field.\n",
    "- **Construct a U-Net model using PyTorch**: Learn to build a U-Net architecture using PyTorch's convolutional, max-pooling, and upsampling layers.\n",
    "- **Train and evaluate your model for accurate segmentation**: Develop the ability to train your model and assess its performance in automating image segmentation tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Setup](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "- **pandas**: Used for managing and manipulating structured data efficiently.\n",
    "- **numpy**: Essential for numerical operations. \n",
    "- **matplotlib**: A comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
    "- **opencv-python-headless**: This is a lightweight version of the OpenCV library, which provides tools for image processing and computer vision.\n",
    "- **torch (PyTorch)**: A flexible and powerful deep learning library that provides rich APIs for deep learning algorithms and neural network architectures.\n",
    "- **torchvision**: Works in conjunction with PyTorch, providing popular datasets, model architectures, and common image transformations for computer vision. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_1_'></a>[Installing required libraries](#toc0_)\n",
    "\n",
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You must run the following cell__ to install them. Please wait until it completes.\n",
    "\n",
    "This step could take **several minutes**, please be patient.\n",
    "\n",
    "**NOTE**: If you encounter any issues, please restart the kernel and run again.  You can do that by clicking the **Restart the kernel** icon.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/crvBKBOkg9aBzXZiwGEXbw/Restarting-the-Kernel.png\" width=\"50%\" alt=\"Restart kernel\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4\n",
    "!pip install pandas==2.2.2\n",
    "!pip install matplotlib==3.9.0\n",
    "!pip install opencv-python-headless==4.10.0.82\n",
    "!pip install torch==2.3.1\n",
    "!pip install torchvision==0.18.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_2_'></a>[Importing required libraries](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2 as cv\n",
    "import zipfile\n",
    "import io\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# You can use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_3_'></a>[Import data](#toc0_)\n",
    "\n",
    "The dataset used here originates from the [2012 ISBI challenge](https://imagej.net/events/isbi-2012-segmentation-challenge), specifically designed for the segmentation of neuronal structures within electron microscopic stacks. \n",
    "\n",
    "**Training Data**\n",
    "\n",
    "The training data is a set of 30 sections from a serial section Transmission Electron Microscopy (ssTEM) dataset of the Drosophila first instar larva ventral nerve cord (VNC). The microcube measures 2 x 2 x 1.5 microns approx., with a resolution of 4x4x50 nm/pixel.\n",
    "\n",
    "The corresponding binary labels are provided in an in-out fashion, i.e. white for the pixels of segmented objects and black for the rest of pixels (which correspond mostly to membranes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the link to the ZIP file\n",
    "zip_file_url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/o_kRuP_Bk_Pa2xvo1ly63w/data.zip'\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "    \"\"\"\n",
    "        Normalize the image to be in range 0 to 1\n",
    "\n",
    "        Args:\n",
    "            image: The input image\n",
    "        Returns:\n",
    "            The normalized image\n",
    "    \"\"\"\n",
    "    return np.array(image) / 255.0\n",
    "\n",
    "\n",
    "def center_crop(img, output_size):\n",
    "    \"\"\"\n",
    "        Crop the image from the center\n",
    "        \n",
    "        Args:\n",
    "            img: The input image\n",
    "            output_size: The size of the output image\n",
    "        Returns:\n",
    "            The cropped image\n",
    "    \"\"\"\n",
    "    img = np.array(img)\n",
    "\n",
    "    start_y = (img.shape[0] - output_size[0]) // 2\n",
    "    start_x = (img.shape[1] - output_size[1]) // 2\n",
    "\n",
    "    cropped_img = img[start_y:start_y + output_size[0], start_x:start_x + output_size[1]]\n",
    "    return cropped_img\n",
    "\n",
    "\n",
    "def shape_process(img, input_size):\n",
    "    \"\"\"\n",
    "    Resize the image if its height is odd and pad it to match the input size.\n",
    "    \n",
    "    Args:\n",
    "        img (numpy array): The input image.\n",
    "        input_size (tuple): The desired size of the output image as a tuple (height, width).\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: The processed image with the specified input size.\n",
    "    \"\"\"\n",
    "    if img.shape[0] % 2:\n",
    "        img = cv.resize(img, img.shape[0] // 2, interpolation=cv.INTER_CUBIC)\n",
    "    padd = int((input_size[0] - img.shape[0]) / 2)\n",
    "    img = cv.copyMakeBorder(img, padd, padd, padd, padd, cv.BORDER_REFLECT_101)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(zip_file_url)\n",
    "zip_file_bytes = io.BytesIO(response.content)\n",
    "\n",
    "# Initialize lists to hold images and labels\n",
    "test_images = []\n",
    "train_labels = []\n",
    "train_images = []\n",
    "\n",
    "with zipfile.ZipFile(zip_file_bytes, 'r') as zip_ref:\n",
    "    sorted_files = sorted(zip_ref.namelist())\n",
    "    for file_name in sorted_files:\n",
    "        if file_name.endswith('.tif'):\n",
    "            with zip_ref.open(file_name) as file:\n",
    "                img = Image.open(file).convert('L')  # Convert to grayscale\n",
    "                img = np.array(img)\n",
    "                if '/train/images/' in file_name:\n",
    "                    train_images.append(normalize(shape_process(img, (572, 572))))\n",
    "                elif '/train/labels/' in file_name:\n",
    "                    cropped_img = center_crop(img, (388, 388))\n",
    "                    normalized_img = normalize(cropped_img)\n",
    "                    mask = (normalized_img > 0.5).astype(int)\n",
    "                    train_labels.append(mask)\n",
    "                elif '/test/' in file_name:\n",
    "                    test_images.append(normalize(shape_process(img, (572, 572))))\n",
    "\n",
    "    train_images = np.asarray(train_images, dtype=np.float32)\n",
    "    # [N, H, W, C]\n",
    "    train_images = train_images[:, :, :, np.newaxis]\n",
    "\n",
    "    test_images = np.asarray(test_images, dtype=np.float32)\n",
    "    test_images = test_images[:, :, :, np.newaxis]\n",
    "\n",
    "    train_labels = np.asarray(train_labels, dtype=np.float32)\n",
    "    train_labels = train_labels[:, :, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the shape of: `train_images`, `train_labels`, and `test_images`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train_images.shape: {train_images.shape}')\n",
    "print(f'train_labels.shape: {train_labels.shape}')\n",
    "print(f'test_images.shape: {test_images.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check some sample images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please try different index here, e.g. 1, 2, 3, ..., 29.\n",
    "sample_image_index = 1\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.squeeze(center_crop(train_images[sample_image_index], (388, 388))), cmap='gray')\n",
    "plt.title(f'Train Image: {sample_image_index}')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.squeeze(train_labels[sample_image_index]), cmap='gray')\n",
    "plt.title(f'Train Image (Ground Truth): {sample_image_index}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ Left Image: This is one of the training images from the dataset. It shows a high-resolution transmission electron microscopy (TEM) image of the Drosophila first instar larva ventral nerve cord (VNC). The detailed structures visible in the image represent the cellular and subcellular components of the neuronal tissue.\n",
    "+ Right Image: This is the ground truth annotation corresponding to the left image. It is a binary mask, where:\n",
    "  + White areas represent the segmented objects, which in this context are the neuronal structures.\n",
    "  + Black areas represent the rest of the pixels, primarily corresponding to cell membranes and other non-neuronal structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_4_'></a>[U-Net architecture](#toc0_)\n",
    "\n",
    "U-Net is renowned for its symmetric encoder-decoder structure, which is pivotal for effective image segmentation, particularly in medical imaging. This architecture enables the model to process images through a series of transformations, balancing both high and low-resolution information throughout the network.\n",
    "\n",
    "#### <a id='toc1_5_4_1_'></a>[Encoder (Contracting path)](#toc0_)\n",
    "\n",
    "- **Downsampling**: The encoder part of the network uses convolutional blocks to downsample the image. Each block typically consists of:\n",
    "  - **Convolutional Layers**: Apply filters to extract features.\n",
    "  - **Non-linear Activation Layers**: Typically ReLU (Rectified Linear Unit) to introduce non-linearities into the model, enhancing its ability to learn complex patterns.\n",
    "  - **Max-Pooling Layers**: Reduce the spatial dimensions of the feature maps, increasing the field of view of the filters and thus focusing on the most prominent features.\n",
    "\n",
    "#### <a id='toc1_5_4_2_'></a>[Decoder (Expanding path)](#toc0_)\n",
    "\n",
    "- **Upsampling**: The decoder uses transposed convolutions to symmetrically upsample the downsampled feature map to reconstruct a high-resolution feature map that approaches the size of the original image.\n",
    "- **Concatenation**: One of the distinctive features of U-Net is the concatenation of high-resolution feature maps from the contracting path with the corresponding upsampled feature maps from the expanding path. This step is crucial as it combines contextual information gathered during downsampling with the spatial information recovered during upsampling, allowing for precise localization and detailed segmentation.\n",
    "\n",
    "#### <a id='toc1_5_4_3_'></a>[Output layer](#toc0_)\n",
    "\n",
    "- **Convolutional Output**: Instead of using fully connected layers, U-Net employs a final convolutional layer to output the segmentation map. Each filter in this layer correlates with a specific object class, enabling the network to segment multiple classes within an image simultaneously.\n",
    "\n",
    "#### <a id='toc1_5_4_4_'></a>[Advantages of U-Net architecture](#toc0_)\n",
    "\n",
    "- **Efficient Use of Data**: By leveraging feature concatenation, the network effectively uses both the high-resolution features and the abstracted low-resolution context, which is especially beneficial in scenarios with limited data availability.\n",
    "- **Flexibility**: The use of convolutional output layers instead of fully connected layers not only reduces the number of trainable parameters, but also enhances the network’s ability to handle images of varying sizes.\n",
    "\n",
    "This architecture has set a standard in medical image segmentation tasks, proving particularly effective in scenarios requiring precise localization and detailed contextual understanding of the images.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Kk-yKGkvRtkNzLkZdP3Pag/unet.png\" width=\"70%\" alt=\"indexing\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Downsampling path\n",
    "        self.conv1 = self.double_conv(1, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = self.double_conv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = self.double_conv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = self.double_conv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = self.double_conv(512, 1024)\n",
    "\n",
    "        # Upsampling path\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        # concat conv4 + up_conv1 \n",
    "        self.conv6 = self.double_conv(512 + 512, 512)\n",
    "\n",
    "        self.up_conv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        # concat conv4 + up_conv1 \n",
    "        self.conv6 = self.double_conv(512 + 512, 512)\n",
    "\n",
    "        self.up_conv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        # concat conv3 + up_conv2\n",
    "        self.conv7 = self.double_conv(256 + 256, 256)\n",
    "\n",
    "        self.up_conv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        # concat conv2 + up_conv3\n",
    "        self.conv8 = self.double_conv(128 + 128, 128)\n",
    "\n",
    "        self.up_conv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        # concat conv1 + up_conv4\n",
    "        self.conv9 = self.double_conv(64 + 64, 64)\n",
    "\n",
    "        self.conv10 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1)\n",
    "\n",
    "    def double_conv(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def center_crop(self, source_tensor, target_shape):\n",
    "        start_y = (source_tensor.shape[2] - target_shape[2]) // 2\n",
    "        start_x = (source_tensor.shape[3] - target_shape[3]) // 2\n",
    "\n",
    "        return source_tensor[:, :, start_y:start_y + target_shape[2], start_x:start_x + target_shape[3]]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downsampling path\n",
    "        c1 = self.conv1(x)\n",
    "        p1 = self.pool1(c1)\n",
    "\n",
    "        c2 = self.conv2(p1)\n",
    "        p2 = self.pool2(c2)\n",
    "\n",
    "        c3 = self.conv3(p2)\n",
    "        p3 = self.pool3(c3)\n",
    "\n",
    "        c4 = self.conv4(p3)\n",
    "        p4 = self.pool4(c4)\n",
    "\n",
    "        x = self.conv5(p4)\n",
    "\n",
    "        x = self.up_conv1(x)\n",
    "        x = torch.cat((self.center_crop(c4, x.shape), x), dim=1)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x = self.up_conv2(x)\n",
    "        x = torch.cat((self.center_crop(c3, x.shape), x), dim=1)\n",
    "        x = self.conv7(x)\n",
    "\n",
    "        x = self.up_conv3(x)\n",
    "        x = torch.cat((self.center_crop(c2, x.shape), x), dim=1)\n",
    "        x = self.conv8(x)\n",
    "\n",
    "        x = self.up_conv4(x)\n",
    "        x = torch.cat((self.center_crop(c1, x.shape), x), dim=1)\n",
    "        x = self.conv9(x)\n",
    "\n",
    "        output = torch.sigmoid(self.conv10(x))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_5_'></a>[Display the U-Net model structure](#toc0_)\n",
    "\n",
    "To understand the internal architecture of our U-Net model, we can visualize its configuration by printing it. This will display the layers, their order, and configurations, providing insights into how data flows through the network. The following cell executes this visualization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up the computation device**\n",
    "\n",
    "To optimize the performance of our neural network, it's crucial to use the available hardware acceleration. The following code checks if a CUDA-capable GPU (Graphics Processing Unit) is available. If a GPU is available, it sets the device to 'cuda' to enable GPU acceleration with PyTorch. Otherwise, it falls back to using the CPU (Central Processing Unit). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting layer output sizes in the U-Net model**\n",
    "\n",
    "Understanding the dimensions of data as it propagates through the different layers of a neural network is crucial for debugging and optimizing the model architecture. This part of the notebook demonstrates how to programmatically inspect the output sizes of each layer in our U-Net model. \n",
    "\n",
    "Here's what each part of the code does:\n",
    "\n",
    "1. **Creates a random input tensor**. \n",
    "\n",
    "2. **Defines a hook function**:\n",
    "+ A hook function in PyTorch can be used to perform actions during the forward or backward pass.\n",
    "+ This function prints the name of the module and the size of its output.\n",
    "\n",
    "3. **Registers the hook**:\n",
    "+ For each layer, it registers the print_size function as a forward hook using register_forward_hook.\n",
    "+ This ensures that print_size will be called during the forward pass of each layer, printing the output size.\n",
    "\n",
    "4. **Performs a forward pass without gradient computation**:\n",
    "+ The torch.no_grad() context manager is used to disable gradient calculation, which reduces memory usage and speeds up computations during inference.\n",
    "+ The input tensor is passed through the model, and the final output is computed and stored in the variable output.\n",
    "+ During this forward pass, the print_size hook function will be called for each layer, printing the output size at each step.\n",
    "\n",
    "5. **Prints the final output size**:\n",
    "\n",
    "This procedure is particularly useful for verifying the expected transformations at each stage of the model, ensuring that all layers are correctly configured and that the input dimensions are suitable for the network architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(1, 1, 572, 572)\n",
    "\n",
    "\n",
    "def print_size(module, input, output):\n",
    "    print(f\"{module.__class__.__name__} output size: {output.size()}\")\n",
    "\n",
    "\n",
    "for layer in model.children():\n",
    "    layer.register_forward_hook(print_size)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "print(\"Final output size:\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating data loaders for training**\n",
    "\n",
    "In PyTorch, data loaders are an essential component that simplify data manipulation for training and testing machine learning models. They handle batching, shuffling, and loading the data during the training process, which is vital for efficiency and effectiveness. Below, we will detail the steps involved in preparing these loaders for training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 1\n",
    "\n",
    "# Create tensors for the first 6 images (3 for training, 3 for validation)\n",
    "train_tensor = torch.from_numpy(train_images[:6]).permute(0, 3, 1, 2).float().to(device)\n",
    "train_groundtruth_tensor = torch.from_numpy(train_labels[:6]).permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "dataset = TensorDataset(train_tensor, train_groundtruth_tensor)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the split sizes\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "# train_dataset = TensorDataset(train_tensor, train_groundtruth_tensor)\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_6_'></a>[Training the U-Net model](#toc0_)\n",
    "\n",
    "The code below outlines the process of setting up and executing the training loop.\n",
    "\n",
    "1. **Model setup**:\n",
    "   - `model = UNet().to(device)`: Initializes the U-Net model and moves it to the appropriate device (CPU or GPU), which is determined by the `device` variable. \n",
    "\n",
    "2. **Optimizer**:\n",
    "   - `optim = torch.optim.Adam(model.parameters(), lr=0.0001)`: Sets up the Adam optimizer for the model with a learning rate of 0.0001.\n",
    "\n",
    "3. **Training epochs**:\n",
    "   - `epochs = 5`: Specifies that the model should be trained over 5 epochs. An epoch is a complete pass over the entire training dataset.\n",
    "\n",
    "**Note**: The following code just shows how to configure the training with a few epochs and images. For comprehensive training, typically 300 epochs and all 30 images from the dataset would be used. We will also provide a pre-trained model for you to use later.\n",
    "\n",
    "Please note that this training session may take **6~7 minutes** to complete. Please be patient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = UNet().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "epochs = 5\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training and validation loop\n",
    "for e in tqdm(range(epochs), leave=False, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0  # To accumulate loss for each epoch\n",
    "\n",
    "    for data, labels in tqdm(train_dl, leave=False, desc=\"   Training Dataset\"):\n",
    "        optim.zero_grad()\n",
    "        prediction = model(data)\n",
    "        loss = F.mse_loss(prediction, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        epoch_train_loss += loss.item()  # Summing up the loss for the epoch\n",
    "\n",
    "    average_train_loss = epoch_train_loss / len(train_dl)\n",
    "    train_losses.append(average_train_loss)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0  # To accumulate loss for validation\n",
    "    with torch.no_grad():\n",
    "        for data, labels in tqdm(val_dl, leave=False, desc=\"   Validation Dataset\"):\n",
    "            prediction = model(data)\n",
    "            loss = F.mse_loss(prediction, labels)\n",
    "            epoch_val_loss += loss.item()  # Summing up the loss for the epoch\n",
    "\n",
    "    average_val_loss = epoch_val_loss / len(val_dl)\n",
    "    val_losses.append(average_val_loss)\n",
    "\n",
    "    print(f\"Epoch {e+1}, Train Loss: {average_train_loss:.4f}, Val Loss: {average_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will visualize the model's performance by plotting the loss curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the epoch-wise loss data, there is a **decrease** in loss values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_7_'></a>[Loading and testing a pre-trained U-Net model](#toc0_)\n",
    "\n",
    "To demonstrate the application of a pre-trained U-Net model, we will perform image segmentation on a sample test image. This example uses a model that has been trained previously and stored remotely. We will load this model and use it for inference to highlight the model's capabilities, particularly in processing biomedical images. Since we're using the CPU for this example, the process is more for demonstration rather than optimized performance, which would typically require a GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pt_file(url):\n",
    "    response = requests.get(url)\n",
    "    return io.BytesIO(response.content)\n",
    "\n",
    "\n",
    "pt_file_url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Iu482aowoEaP9MgNqhyJ_w/model-epo300.pt'\n",
    "state_dict = torch.load(download_pt_file(pt_file_url), map_location=torch.device(device))\n",
    "\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not have ground truth for the testing data, we will visualize the predictions on the training data and compare them with the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different sample_image_index here.\n",
    "sample_image_index = 1\n",
    "\n",
    "predictions = []\n",
    "test_tensor = torch.from_numpy(train_images[sample_image_index]).unsqueeze(0).permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(test_tensor)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np.squeeze(center_crop(train_images[sample_image_index], (388, 388))), cmap='gray')\n",
    "plt.title(f'Train Image: {sample_image_index}')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(make_grid(prediction)[0].numpy(), cmap='gray')\n",
    "plt.title(f'Train Image (Prediction): {sample_image_index}')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(np.squeeze(train_labels[sample_image_index]), cmap='gray')\n",
    "plt.title(f'Train Image (Ground Truth): {sample_image_index}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the prediction closely resembles the ground truth. \n",
    "\n",
    "Now, let us try some test images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_index = 1\n",
    "\n",
    "predictions = []\n",
    "test_tensor = torch.from_numpy(test_images[test_image_index]).unsqueeze(0).permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(test_tensor)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.squeeze(center_crop(test_images[test_image_index], (388, 388))), cmap='gray')\n",
    "plt.title(f'Test Image: {test_image_index}')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(make_grid(prediction)[0].numpy(), cmap='gray')\n",
    "plt.title(f'Predicted Image: {test_image_index}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Exercises](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_1_'></a>[Exercise - Try different test images](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the image_index here.\n",
    "test_image_index = 10 # Change this to any number between 0 and 29\n",
    "\n",
    "predictions = []\n",
    "test_tensor = torch.from_numpy(test_images[test_image_index]).unsqueeze(0).permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(test_tensor)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.squeeze(center_crop(test_images[test_image_index], (388, 388))), cmap='gray')\n",
    "plt.title(f'Test Image: {test_image_index}')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(make_grid(prediction)[0].numpy(), cmap='gray')\n",
    "plt.title(f'Predicted Image: {test_image_index}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "test_image_index = 6\n",
    "test_image_index = 7\n",
    "test_image_index = 8\n",
    "test_image_index = 9\n",
    "test_image_index = 10\n",
    "...\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use another dataset: [BraTS2020 Dataset (Training + Validation)](https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation/data).\n",
    "\n",
    "\n",
    "**Imaging Data Description**\n",
    "\n",
    "All BraTS multimodal scans are available as NIfTI files (.nii.gz) and describe a) native (T1) and b) post-contrast T1-weighted (T1Gd), c) T2-weighted (T2), and d) T2 Fluid Attenuated Inversion Recovery (T2-FLAIR) volumes, and were acquired with different clinical protocols and various scanners from multiple (n=19) institutions, mentioned as data contributors here.\n",
    "\n",
    "All the imaging datasets have been segmented manually, by one to four raters, following the same annotation protocol, and their annotations were approved by experienced neuro-radiologists. Annotations comprise the GD-enhancing tumor (ET — label 4), the peritumoral edema (ED — label 2), and the necrotic and non-enhancing tumor core (NCR/NET — label 1), as described both in the BraTS 2012-2013 TMI paper and in the latest BraTS summarizing paper. The provided data are distributed after their pre-processing, i.e., co-registered to the same anatomical template, interpolated to the same resolution (1 mm^3) and skull-stripped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-image==0.24.0\n",
    "%pip install nibabel==5.2.1\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import PIL\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data\n",
    "# from skimage.util import montage \n",
    "# import skimage.transform as skTrans\n",
    "# from skimage.transform import rotate\n",
    "# from skimage.transform import resize\n",
    "# from PIL import Image, ImageOps  \n",
    "import nibabel as nib\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "extract_dir = \"extracted_files\"\n",
    "if not os.path.exists(extract_dir):\n",
    "    url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/EFOEudjvmSIMYp700qwtrg/dataset.zip'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    zip_file = io.BytesIO(response.content)\n",
    "\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        # Extract all contents to a specific directory\n",
    "        zip_ref.extractall(\"extracted_files\")\n",
    "\n",
    "extracted_files = os.listdir(\"extracted_files\")\n",
    "print(\"Extracted files:\", extracted_files)\n",
    "\n",
    "TRAIN_DATASET_PATH = 'extracted_files/dataset/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\n",
    "VALIDATION_DATASET_PATH = 'extracted_files/dataset/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n",
    "\n",
    "test_image_flair=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_flair.nii').get_fdata()\n",
    "test_image_t1=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t1.nii').get_fdata()\n",
    "test_image_t1ce=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t1ce.nii').get_fdata()\n",
    "test_image_t2=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t2.nii').get_fdata()\n",
    "test_mask=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_seg.nii').get_fdata()\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize = (20, 10))\n",
    "slice_w = 25\n",
    "ax1.imshow(test_image_flair[:,:,test_image_flair.shape[0]//2-slice_w], cmap = 'gray')\n",
    "ax1.set_title('Image flair')\n",
    "ax2.imshow(test_image_t1[:,:,test_image_t1.shape[0]//2-slice_w], cmap = 'gray')\n",
    "ax2.set_title('Image t1')\n",
    "ax3.imshow(test_image_t1ce[:,:,test_image_t1ce.shape[0]//2-slice_w], cmap = 'gray')\n",
    "ax3.set_title('Image t1ce')\n",
    "ax4.imshow(test_image_t2[:,:,test_image_t2.shape[0]//2-slice_w], cmap = 'gray')\n",
    "ax4.set_title('Image t2')\n",
    "ax5.imshow(test_mask[:,:,test_mask.shape[0]//2-slice_w])\n",
    "ax5.set_title('Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's [construct](https://www.kaggle.com/code/mrmohammadi/2d-unet-pytorch/notebook) a U-Net model for the BraTS dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def double_convolution(in_channels, out_channels):\n",
    "    \"\"\"\n",
    "    In the original paper implementation, the convolution operations were\n",
    "    not padded but we are padding them here. This is because, we need the\n",
    "    output result size to be same as input size.\n",
    "    \"\"\"\n",
    "    conv_op = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "    return conv_op\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Contracting path.\n",
    "        # Each convolution is applied twice.\n",
    "        self.down_convolution_1 = double_convolution(1, 64)\n",
    "        self.down_convolution_2 = double_convolution(64, 128)\n",
    "        self.down_convolution_3 = double_convolution(128, 256)\n",
    "        self.down_convolution_4 = double_convolution(256, 512)\n",
    "        self.down_convolution_5 = double_convolution(512, 1024)\n",
    "\n",
    "        # Expanding path\n",
    "        self.up_transpose_1 = nn.ConvTranspose2d(\n",
    "            in_channels=1024, out_channels=512,\n",
    "            kernel_size=2,\n",
    "            stride=2)\n",
    "        \n",
    "        self.up_convolution_1 = double_convolution(1024, 512)\n",
    "        self.up_transpose_2 = nn.ConvTranspose2d(\n",
    "            in_channels=512, out_channels=256,\n",
    "            kernel_size=2,\n",
    "            stride=2)\n",
    "        self.up_convolution_2 = double_convolution(512, 256)\n",
    "        self.up_transpose_3 = nn.ConvTranspose2d(\n",
    "            in_channels=256, out_channels=128,\n",
    "            kernel_size=2,\n",
    "            stride=2)\n",
    "        self.up_convolution_3 = double_convolution(256, 128)\n",
    "        self.up_transpose_4 = nn.ConvTranspose2d(\n",
    "            in_channels=128, out_channels=64,\n",
    "            kernel_size=2,\n",
    "            stride=2)\n",
    "        self.up_convolution_4 = double_convolution(128, 64)\n",
    "        # output => `out_channels` as per the number of classes.\n",
    "        self.out = nn.Conv2d(\n",
    "            in_channels=64, out_channels=num_classes,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_1 = self.down_convolution_1(x)\n",
    "        down_2 = self.max_pool2d(down_1)\n",
    "        down_3 = self.down_convolution_2(down_2)\n",
    "        down_4 = self.max_pool2d(down_3)\n",
    "        down_5 = self.down_convolution_3(down_4)\n",
    "        down_6 = self.max_pool2d(down_5)\n",
    "        down_7 = self.down_convolution_4(down_6)\n",
    "        down_8 = self.max_pool2d(down_7)\n",
    "        down_9 = self.down_convolution_5(down_8)\n",
    "\n",
    "        up_1 = self.up_transpose_1(down_9)\n",
    "        up_2 = self.up_convolution_1(torch.cat([down_7, up_1], 1))\n",
    "        up_3 = self.up_transpose_2(up_2)\n",
    "        up_4 = self.up_convolution_2(torch.cat([down_5, up_3], 1))\n",
    "        up_5 = self.up_transpose_3(up_4)\n",
    "        up_6 = self.up_convolution_3(torch.cat([down_3, up_5], 1))\n",
    "        up_7 = self.up_transpose_4(up_6)\n",
    "        up_8 = self.up_convolution_4(torch.cat([down_1, up_7], 1))\n",
    "\n",
    "        out = self.out(up_8)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training the model from scratch is computationally expensive, we will provide a pre-trained model for you to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(num_classes=1)\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# lists of directories with studies\n",
    "train_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n",
    "\n",
    "\n",
    "def pathListIntoIds(dirList):\n",
    "    x = []\n",
    "    for i in range(0,len(dirList)):\n",
    "        x.append(dirList[i][dirList[i].rfind('/')+1:])\n",
    "    return x\n",
    "\n",
    "train_and_test_ids = pathListIntoIds(train_and_val_directories);\n",
    "\n",
    "train_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.3, random_state=42)\n",
    "train_ids, test_ids = train_test_split(train_test_ids,test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_ids)} | Validation: {len(val_ids)} | Test: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(ids, path):\n",
    "    images = np.zeros((len(ids)*10, 240, 240), np.float32)\n",
    "    masks = np.zeros((len(ids)*10, 240, 240), np.float32)\n",
    "\n",
    "    i = 0\n",
    "    for id in ids:\n",
    "        t2 = nib.load(f\"{path}{id}/{id}_t2.nii\").get_fdata()\n",
    "        seg = nib.load(f\"{path}{id}/{id}_seg.nii\").get_fdata()\n",
    "\n",
    "        for s in range(50, seg.shape[2]-50, 10):\n",
    "            images[i] = t2[:, :, s] / t2.max()\n",
    "            masks[i] = seg[:, :, s] > 0\n",
    "            i += 1\n",
    "\n",
    "    images = np.expand_dims(images[:i], axis=1)\n",
    "    masks = np.expand_dims(masks[:i], axis=1)\n",
    "\n",
    "    return images, masks\n",
    "\n",
    "train_images, train_masks = load_dataset(train_ids, TRAIN_DATASET_PATH)\n",
    "val_images, val_masks = load_dataset(val_ids, TRAIN_DATASET_PATH)\n",
    "\n",
    "train_images.shape, val_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_images).type(torch.float32), torch.from_numpy(train_masks).type(torch.float32))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(val_images).type(torch.float32), torch.from_numpy(val_masks).type(torch.float32))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X, Y = next(iter(train_dataloader))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X[0, 0], cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(Y[0, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the pre-trained model to make predictions.\n",
    "\n",
    "In case there are some issues with the following code due to the platform's capability, I will show the sample result here:\n",
    "\n",
    "![results.jpg](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/wuD3axkV8qCl79ZSZE4U6g/results.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "\n",
    "checkpoint_url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HATpl02lA0ykn9aAU9K6sA/checkpoint-epoch-29.pt'\n",
    "local_checkpoint_path = 'checkpoint-epoch-29.pt'\n",
    "\n",
    "if not os.path.exists(local_checkpoint_path):\n",
    "    response = requests.get(checkpoint_url)\n",
    "    # Save the downloaded file locally\n",
    "    with open(local_checkpoint_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(local_checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Reinitialize the model architecture\n",
    "model = UNet(num_classes=1).to(torch.device('cpu'))\n",
    "\n",
    "# Load the model state dictionary from the checkpoint\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "X, y = next(iter(test_dataloader))\n",
    "X, y = X.to(torch.device('cpu')), y.to(torch.device('cpu'))\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_pred = model(X)\n",
    "\n",
    "y1 = y[3, 0].cpu().detach().numpy()\n",
    "y2 = y_pred[3, 0].cpu().detach().numpy()\n",
    "plt.subplot(121)\n",
    "plt.imshow(y1, cmap='gray')\n",
    "plt.title('Ground Truth')\n",
    "plt.subplot(122)\n",
    "plt.imshow(y2, cmap='gray')\n",
    "plt.title('Prediction')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_color = np.zeros((*y1.shape, 3))\n",
    "y_color[..., 0] = y1\n",
    "y_color[..., 1] = y2\n",
    "y_color[..., 2] = y2\n",
    "\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(X[4, 0].cpu().detach().numpy())\n",
    "plt.subplot(122)\n",
    "plt.imshow(y_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Authors](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ricky Shi](https://author.skills.network/instructors/ricky_shi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Contributors](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wojciech \"Victor\" Fulmyk](https://www.linkedin.com/in/wfulmyk)\n",
    "\n",
    "[Kang Wang](https://author.skills.network/instructors/kang_wang)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2024 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "d3719fb16fc471c87467a127d136dfdcc3993df043e2150a3f2b7c43b1a142e9"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
