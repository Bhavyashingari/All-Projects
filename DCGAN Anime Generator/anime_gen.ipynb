{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "dfe8624b-c214-4e97-b62f-05108043ebb5",
      "cell_type": "markdown",
      "source": "SIMPLE GANS",
      "metadata": {}
    },
    {
      "id": "75f517d7-3d15-4b7b-9080-478777468bfb",
      "cell_type": "code",
      "source": "import warnings\nwarnings.simplefilter('ignore')\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom  torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch.optim import lr_scheduler\nfrom tqdm import tqdm\nfrom IPython import display\n\nimport torchvision.utils as vutils\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom PIL import Image\nfrom torchvision.transforms import ToPILImage\n\n\nimport os\nfrom os import listdir\nfrom pathlib import Path\nimport imghdr\nimport skillsnetwork",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b9509f16-84e2-470c-ba7b-cdc6abbbb6e0",
      "cell_type": "code",
      "source": "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0A52EN/G_trained.pth\n!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0A52EN/D_trained.pth",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "af6495c0-21e7-4e44-85e5-9e025503dc5f",
      "cell_type": "code",
      "source": "# This function will allow us to easily plot data taking in x values, y values, and a title\ndef plot_distribution(real_data,generated_data,discriminator=None,density=True):\n    \n    plt.hist(real_data.numpy(), 100, density=density, facecolor='g', alpha=0.75, label='real data')\n    plt.hist(generated_data.numpy(), 100, density=density, facecolor='r', alpha=0.75,label='generated data q(z) ')\n    \n    if discriminator:\n        max_=torch.max(real_data.max(),generated_data.max().detach())\n        min_=torch.min(real_data.min(),generated_data.min().detach())\n        x=torch.linspace(start=min_, end=max_, steps=100)\n        plt.plot(x.numpy(),discriminator(x.view(-1,1)).detach().view(-1).numpy(),label='discriminator',color='k')\n        plt.plot(x.numpy(),0.5*np.ones(x.shape),label='0.5',color='b')\n        plt.legend()\n        plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "33b7b310-5584-467c-a943-957a569919b4",
      "cell_type": "code",
      "source": "# plot batch of images\ndef plot_image_batch(my_batch):\n\n  fig, axes = plt.subplots(nrows=8, ncols=8, figsize=(10, 10))\n  img_num=0\n  for i in range(8):\n      for j in range(8):\n          ax = axes[i][j]\n          img_num+=1\n  \n          ax.imshow(np.transpose(vutils.make_grid(my_batch[img_num].to(device), padding=2, normalize=True).cpu(),(1,2,0)))\n  plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6d7dcd89-1604-4e53-b4a5-62f17da9ee81",
      "cell_type": "code",
      "source": "def get_accuracy(X,Xhat):\n    total=0\n    py_x=D(X)\n    total=py_x.mean()\n    py_x=D(Xhat)\n    total+=py_x.mean()\n    return total/2",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d51c08cd-5a74-4c99-8575-48b009d8d2a6",
      "cell_type": "code",
      "source": "class Dataset(Dataset):\n    def __init__(self, covariance_matrix, mean,n_samples=1000):\n        self.obj =  MultivariateNormal(loc=mean, covariance_matrix=covariance_matrix)\n        self.mean=mean\n        self.covariance_matrix=covariance_matrix\n        self.X=torch.tensor([[self.obj.sample()] for n in range(n_samples)])\n        \n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        \n        return self.X[idx,:]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "875d09db-04f5-4753-9fb4-b3d7cda79d37",
      "cell_type": "code",
      "source": "n_samples=10000\nmean=10*torch.ones(1)\ncovariance_matrix=0.1*torch.eye(1)\n\ndataset=Dataset(covariance_matrix=covariance_matrix, mean=mean,n_samples=n_samples)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ce1ddbe7-c604-411d-9494-6bb28af8ad38",
      "cell_type": "code",
      "source": "Z=torch.randn(n_samples,1)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "be1bf0cb-1ad8-4248-a8e2-5a71cf33a80b",
      "cell_type": "code",
      "source": "print(\"mean:\",Z.mean())\nprint(\"standard deviation:\",Z.std())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "804cb9f8-6007-4e8b-a23f-7ab7d6bbb016",
      "cell_type": "code",
      "source": "Xhat=Z+10",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f17d8dda-c466-401e-b0bc-179e0dbe20ff",
      "cell_type": "code",
      "source": "print(\"mean:\",Xhat.mean())\nprint(\"standard deviation:\",Xhat.std())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "12d2278e-4b63-4651-b3ec-881bce749458",
      "cell_type": "code",
      "source": "plot_distribution(real_data=dataset.X,generated_data=Xhat)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "99b3266b-07ef-4df6-a80a-917f1cb1584e",
      "cell_type": "code",
      "source": "class Generator(nn.Module):\n    def __init__(self,input_dim=1):\n        super(Generator,self).__init__()\n        self.l1=nn.Linear(1,input_dim)\n    \n    def forward(self, x):\n        return self.l1(x)\nG=Generator()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2f6e27ec-9233-4047-9f00-ed819c645c78",
      "cell_type": "code",
      "source": "class Discriminator(nn.Module):\n    def __init__(self,input_dim=1):\n        super(Discriminator,self).__init__()\n        self.l1=nn.Linear(1,input_dim)\n    \n    def forward(self, x):\n        return torch.sigmoid(self.l1(x))\n\nD=Discriminator() ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8f87e0d1-e7b3-4e15-beb2-0a74572ebe54",
      "cell_type": "code",
      "source": "D",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "28c4b5ea-4e72-45ca-8cd4-04e6dd0dbb87",
      "cell_type": "code",
      "source": "n_samples=1000\nXhat=G(torch.randn(n_samples,1))\nplot_distribution(real_data=dataset.X,generated_data=Xhat.detach(),discriminator=D)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5e428c28-e320-4402-a50e-b0c176946f4d",
      "cell_type": "code",
      "source": "py_x=D(dataset.X)\ntorch.sum(py_x>0.5)/len(py_x)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "32131e78-c4e0-400a-b668-7ead34379b32",
      "cell_type": "code",
      "source": "py_x=D(Z)\ntorch.sum(py_x>0.5)/len(py_x)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e3bcdc17-ec19-4e09-9c13-3fb989872193",
      "cell_type": "code",
      "source": "Z",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8adfe1b5-d130-430b-8e4b-00513722e7f8",
      "cell_type": "code",
      "source": "get_accuracy(dataset.X,Xhat)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "92c93369-fd50-498f-8015-9eda2ed89cbf",
      "cell_type": "code",
      "source": "criterion=nn.BCELoss()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d5c98675-1efb-4246-b023-902edcce1e8e",
      "cell_type": "code",
      "source": "criterion(D(G(Z)),torch.zeros(Z.shape[0],1))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3a0de998-8d12-494e-922a-8f0b3f2c314c",
      "cell_type": "code",
      "source": "criterion(D(dataset.X),torch.ones(len(dataset),1))+criterion(D(G(Z)),torch.zeros(Z.shape[0],1))/2",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "03e0bf46-fe37-4557-a05f-773c8421f587",
      "cell_type": "code",
      "source": "# Learning rate for optimizers\nlr = 0.1\n\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.1\n\nG=Generator()\nD=Discriminator()\n\noptimizerG=optim.Adam(G.parameters(),lr=lr,betas=(beta1, 0.999))\noptimizerD=optim.Adam(D.parameters(),lr=lr,betas=(beta1, 0.999))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5e92d7b3-6f4f-42d7-a653-e0b3c5db8036",
      "cell_type": "code",
      "source": "batch_size=100\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "47c86d8f-1953-4ae7-ab93-e1123e159970",
      "cell_type": "code",
      "source": "LOSS_G=[]\nLOSS_D=[]\n\nepochs=20\n\nnoise_dim=1\nepsilon=100 \n\n# Training loop\nfor epoch in tqdm(range(epochs)):\n    # Plot the data distribution and generated samples\n    Xhat=G(torch.randn(n_samples,1))\n    plot_distribution(real_data=dataset.X, generated_data=Xhat.detach(), discriminator=D)\n    \n    # Training the discriminator\n    for real_data in dataloader:\n        noise = torch.randn(batch_size, 1)\n        fake_data = G(noise)\n        \n        # Discriminator predictions for real and fake data\n        real_predictions = D(real_data)\n        fake_predictions = D(fake_data)\n \n        # Discriminator loss for real and fake data\n        loss_D_real = criterion(real_predictions, torch.ones(batch_size, 1))\n        loss_D_fake = criterion(fake_predictions, torch.zeros(batch_size, 1))\n        \n        # Overall discriminator loss\n        loss_D = (loss_D_fake + loss_D_real) / 2\n        LOSS_D.append(loss_D.detach().item())\n        \n        # Backpropagation and optimizer update for discriminator\n        D.zero_grad()\n        loss_D.backward(retain_graph=True)\n        optimizerD.step()\n        \n        # Training the generator\n        output = D(fake_data)\n        loss_G = criterion(output, torch.ones(batch_size, 1))\n        LOSS_G.append(loss_G.detach().item())\n    \n        # Backpropagation and optimizer update for generator\n        G.zero_grad()\n        loss_G.backward()\n        optimizerG.step()\n    \n    # Save and display the generator and discriminator if the performance increases \n    Xhat = G(torch.randn(len(dataset), 1))\n    accuracy = abs(0.5 - get_accuracy(dataset.X, Xhat))\n    if accuracy < epsilon:\n        epsilon = accuracy\n        torch.save(D.state_dict(), 'D.pth')\n        torch.save(G.state_dict(), 'G.pth')\n        print(\"Epoch:\", epoch)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a63aeeb2-8896-4d40-ad34-344b4790d754",
      "cell_type": "code",
      "source": "D=Discriminator()\nD.load_state_dict(torch.load(\"D.pth\"))\nG=Generator()\nG.load_state_dict(torch.load(\"G.pth\"))\n\nXhat=G(torch.randn(len(dataset), 1))\nplot_distribution(real_data=dataset.X, generated_data=Xhat.detach(), discriminator=D)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b2dfe070-947e-408d-9a06-5acd060acfdc",
      "cell_type": "markdown",
      "source": "DEEP CONV GANS",
      "metadata": {}
    },
    {
      "id": "6bf496a3-b4c4-48d9-b6c0-6d073dbbacd5",
      "cell_type": "code",
      "source": "img_height, img_width, batch_size=64,64,128",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ab9381be-45bc-4248-9010-a18cbc4bebc1",
      "cell_type": "code",
      "source": "current_directory = os.getcwd()\ndirectory=os.path.join(current_directory ,'cartoon_20000')\n[filename for filename in os.listdir(directory) if filename.endswith('.jpg') ]\n\nclass Dataset(Dataset):\n    def __init__(self, transform=None):\n      current_directory = os.getcwd()\n      directory=os.path.join(current_directory ,'cartoon_20000')\n\n      self.file_paths = [os.path.join(directory,filename ) for filename in os.listdir(directory) if filename.endswith('.jpg') ]\n      self.transform = transform\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, index):\n        image_path = self.file_paths[index]\n        image = Image.open(image_path)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "55487234-d7e8-45c0-bced-1c0ce72a3d0f",
      "cell_type": "code",
      "source": "dataset=Dataset()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "836295e9-dd51-4b8b-94bc-73c7a5981e0c",
      "cell_type": "code",
      "source": "for i in range(1,5):\n  plt.imshow(dataset[i])\n  plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d2eee549-7d56-4581-8571-bfcb601306ef",
      "cell_type": "code",
      "source": "image_size = 64\ntransform=transforms.Compose([\n                               transforms.Resize((64, 64)),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "448b2067-2911-429c-a7c2-79b0450e2945",
      "cell_type": "code",
      "source": "dataset=Dataset(transform=transform)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "253f5a28-d3e0-463d-ba7d-ab240aa83ef3",
      "cell_type": "code",
      "source": "# Create the dataloader\n# batch_size Batch size during training\nbatch_size = 128\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "03da0f49-9e8a-415a-80a6-21f41a10fdfb",
      "cell_type": "code",
      "source": "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\ndevice",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "afea4a24-4d58-49ea-b712-b75e0a783074",
      "cell_type": "code",
      "source": "real_batch = next(iter(dataloader))\nreal_batch.shape",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "042b6114-e691-4011-a4ae-60d6700959f9",
      "cell_type": "code",
      "source": "plot_image_batch(real_batch)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "19f99877-4232-4e35-9c45-4ffe1950e769",
      "cell_type": "code",
      "source": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            # Block 1:input is Z, going into a convolution\n            nn.ConvTranspose2d(latent_vector_size, 64 * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(64 * 8),\n            nn.ReLU(True),\n            # Block 2: input is (64 * 8) x 4 x 4\n            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 4),\n            nn.ReLU(True),\n            # Block 3: input is (64 * 4) x 8 x 8\n            nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 2),\n            nn.ReLU(True),\n            # Block 4: input is (64 * 2) x 16 x 16\n            nn.ConvTranspose2d(64 * 2, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            # Block 5: input is (64) x 32 x 32\n            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # Output: output is (3) x 64 x 64\n        )\n \n    def forward(self, input):\n        output = self.main(input)\n        return output    ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b71e0f3a-3155-4605-8d57-99999e60d7c7",
      "cell_type": "code",
      "source": "latent_vector_size=128\n\nG = Generator().to(device)\nprint(G)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "700407ba-78f4-4459-bc60-2b6eafe74ed9",
      "cell_type": "code",
      "source": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            # Block 1: input is (3) x 64 x 64\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # Block 2: input is (64) x 32 x 32\n            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # Block 3: input is (64*2) x 16 x 16\n            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # Block 4: input is (64*4) x 8 x 8\n            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # Block 5: input is (64*8) x 4 x 4\n            nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid(),\n            nn.Flatten()\n            # Output: 1\n        )\n \n    def forward(self, input):\n        output = self.main(input)\n        return output",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4959982d-bd6d-48f0-9838-dff864865bcc",
      "cell_type": "code",
      "source": "D =Discriminator().to(device)\nprint(D)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5f1100ee-8fec-4bdc-aa25-abd4b1b9381a",
      "cell_type": "code",
      "source": "learning_rate = 0.0002\nG_optimizer = optim.Adam(G.parameters(), lr = learning_rate, betas=(0.5, 0.999))\nD_optimizer = optim.Adam(D.parameters(), lr = learning_rate, betas=(0.5, 0.999))\nscheduler_G = lr_scheduler.StepLR(G_optimizer, step_size=10, gamma=0.1)\nscheduler_D = lr_scheduler.StepLR(D_optimizer, step_size=10, gamma=0.1)\ndevice",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1cb702c4-446e-4362-8b5d-776333df10ef",
      "cell_type": "code",
      "source": "def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n        torch.nn.init.zeros_(m.bias) \n\nD.apply(weights_init)\nG.apply(weights_init)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "82ccb1bf-0e68-4d38-b963-bb2a134a0763",
      "cell_type": "code",
      "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define hyperparameters\nLOSS_G = []\nLOSS_D = []\nepochs = 1\nepsilon = 100\n\n# Training loop\nfor epoch in tqdm(range(epochs)):\n    print(epoch)\n    for real_data in dataloader:\n        real_data = real_data.to(device)\n        noise =torch.randn(batch_size  , latent_vector_size , 1, 1, device=device)\n        fake_data = G(noise)\n        \n        # Discriminator predictions for real and fake data\n        real_predictions = D(real_data)\n        fake_predictions = D(fake_data)\n \n        # Discriminator loss for real and fake data\n        loss_D_real = criterion(real_predictions, torch.ones(len(real_predictions), 1).to(device))\n        loss_D_fake = criterion(fake_predictions, torch.zeros(len(fake_predictions), 1).to(device))\n        \n        # Overall discriminator loss\n        loss_D = (loss_D_fake + loss_D_real) / 2\n        LOSS_D.append(loss_D.detach().item())\n        \n        # Backpropagation and optimizer update for discriminator\n        D.zero_grad()\n        loss_D.backward(retain_graph=True)\n        D_optimizer.step()\n        \n        # Training the generator\n        output = D(fake_data)\n        loss_G = criterion(output, torch.ones(len(output), 1).to(device))\n        LOSS_G.append(loss_G.detach().item())\n    \n        # Backpropagation and optimizer update for generator\n        G.zero_grad()\n        loss_G.backward()\n        G_optimizer.step()\n    \n    # Using LR Scheduler\n    scheduler_G.step()\n    scheduler_D.step()\n    \n    # Displaying Images\n    Xhat = G(noise).to(device).detach()\n    plot_image_batch(Xhat)\n    print(\"Epoch:\", epoch)\n    \n    # Saving the model\n    torch.save(D.state_dict(), 'D.pth')\n    torch.save(G.state_dict(), 'G.pth')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a00c3b21-84dd-4fe9-95d7-f365b9a38fa9",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}